% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reparse_runs.R
\name{reparse_runs}
\alias{reparse_runs}
\title{Combines outputs from varied_runs() together to create a new output that contains different analyses to compare.}
\usage{
reparse_runs(
  run_list,
  method = NULL,
  list_names = as.character(1:length(run_list))
)
}
\arguments{
\item{run_list}{Several different outputs from varied_runs() in a list format that the user wishes to recombine. The outputs from varied_runs() in the list must all have the same number of runs (the same number of datasets must be simulated).}

\item{method}{Character value. Defaults to Null, meaning every method in each varied_runs() output will be included.}

\item{list_names}{Character vector. Must be equal to the number of elements in run_list. Defaults to numeric naming with the first element of run_list being named "1", the second "2" and so on.}
}
\value{
A list of 11 elements with each element bulleted below in order
\itemize{
\item odds_ratio, NA if not applicable to the method used
\item calculated_ate, the calculated effect size estimate between the set exposure and outcome
\item lower_int, the lower bound of the 95\% confidence interval for the calculated_ate
\item upper_int, the upper bound of the 95\% confidence interval for the calculated_ate
\item p_values, the p value for the calculated_ate
\item exp_prevalence, the exposure prevalence if the exposure variable is binary, returns NAs if not
\item out_prevalence, the outcome prevalence if the outcome variable is binary, returns NAs if not
\item sample_population, the number of samples in the dataset. If the analysis requires stratifying or throwing data samples, sample_population will change to reflect that.
\item set_ate, the effect size between the exposure and outcome that is directly set in the DAG object. Acts as the true effect size.
\item over_r, the overdiagnosis rate that was set. defaults to 0
\item under_r, the overdiagnosis rate that was set. defaults to 0
}

each list element will contain x number of columns and y number of rows, where x is the number of different methods tested, and y is the number of simulated data sets created.
}
\description{
The final output will be a list of 11 elements with each element corresponding to a summary statistic, but the columns in each element will now reflect the different circumstances of each scenario set from varied_runs().
}
\examples{
#creates the causal diagram
my_dag = HydeNetwork(~B|A + C|A*B + D|B*C)
#can plot for better visualization
plot(my_dag)

#if we set up a flexible dag like this, we can quickly change the effect size between nodes A and C as well as the prevalence of node A
flex_dag = function(x,y){
  my_dag = setNode(my_dag, A, nodeType = "dbern", prob = x)
  my_dag = setNode(my_dag, B, nodeType = "dnorm", mu = paste0(0.8," * A + ",0 - 0.8*x), tau = 1)
  my_dag = setNode(my_dag, C, nodeType = "dbern", prob = paste0("ilogit(",y," * A + ",set_p(0.3, y*x),")"))
  my_dag = setNode(my_dag, D, nodeType = "dbern", prob = paste0("ilogit(",0.05," * B + ", 1.2," * C + ",set_p(0.25, 1.2 * 0.9 + 0.05 * 0 ),")"))
}

#To analyze the performance of 1 simulated data frame we can create data and analyze it like so:
sim_data = create_data(flex_dag, 10000,  x=0.5, y = 3)

run_result = apply_methods(exposure = "B", outcome = "C", covariates = "A", df = sim_data)
run_result

#However varied_runs() automatically repeats this process a set number of times and stores all results in a matrix. This allows for MCMC analysis.
small_conf_naive = varied_runs(200, flex_dag, exposure = "B", outcome = "C",  n = 10000, x = .5, y = 1)
small_conf_adj = varied_runs(200, flex_dag, exposure = "B", outcome = "C", covariates = "A",  n = 10000, x = .5, y = 1)
large_conf_naive = varied_runs(200, flex_dag, exposure = "B", outcome = "C",  n = 10000, x = .5, y = 3)
large_conf_adj = varied_runs(200, flex_dag, exposure = "B", outcome = "C", covariates = "A",  n = 10000, x = .5, y = 3)

#you can use reparse_runs() to combine matrices of different runs
confounder_comp = reparse_runs(list(small_conf_naive, small_conf_adj, large_conf_naive, large_conf_adj), list_names =  c("A->C = 1, naive", "A->C = 1, adjusted", "A->C = 3, naive", "A->C = 3, adjusted"))
#ci_ridges lets you quickly visualize results
ci_ridges(confounder_comp, title = "How well is the confounder adjusted for?", subtitle = "Comparison of confounder adjustment methods across different confounder strengths")
#beta_summary() will return summary statistics about the bias present in each scenario
beta_summary(confounder_comp)

#You can also use reparse_runs() to only look at one method in particular.
logistic_comp = reparse_runs(list(small_conf_naive, small_conf_adj, large_conf_naive, large_conf_adj), method = "logistic_regression", list_names =  c("A->C = 1, naive", "A->C = 1, confounder adjusted", "A->C = 3, naive", "A->C = 3, confounder adjusted"))
ci_ridges(logistic_comp, "How well is the confounder adjusted for?", subtitle = "Comparison of logistic regression across different confounder strengths")


#if a beta value between 2 binary nodes is too large, or the sample size is too small, one node may become completely predictive of another which violates separation.
#An error will occur if this is the case
very_large_conf = varied_runs(200, flex_dag, exposure = "B", outcome = "C",  n = 10000, x = .5, y = 100)
}
\seealso{
\code{\link[=apply_methods]{apply_methods()}},\code{\link[=ci_ridges]{ci_ridges()}},\code{\link[=beta_summary]{beta_summary()}}, \code{\link[=varied_runs]{varied_runs()}}
}
